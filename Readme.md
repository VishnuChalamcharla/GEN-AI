# ğŸ¤– Product Catalogue Assistant (RAG-Based)

This project is a **Retrieval-Augmented Generation (RAG)** based Product Catalogue Assistant built using **Python**.  
It allows users to ask natural language questions and get accurate answers **only from product catalog PDFs**, using embeddings and semantic search.

---

## ğŸš€ Features

- ğŸ“„ Ingest product catalog PDFs
- ğŸ” Semantic search using vector embeddings
- ğŸ§  Context-aware question answering
- âŒ Avoids hallucinations by answering strictly from retrieved context
- ğŸ§© Modular and clean code structure
- ğŸ” Environment-based configuration

---

## ğŸ—ï¸ Project Structure



.
â”œâ”€â”€ app.py # Main application entry point
â”œâ”€â”€ ingestion.py # PDF ingestion & chunking
â”œâ”€â”€ Embedding.py # Embedding generation logic
â”œâ”€â”€ Retrieval.py # Vector search & retrieval
â”œâ”€â”€ History_aware.py # Chat history aware retrieval
â”œâ”€â”€ answer_gen.py # LLM-based answer generation
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # Environment variables (NOT committed)
â””â”€â”€ README.md # Project documentation


---

## âš™ï¸ Tech Stack

- **Python 3.9+**
- **LangChain**
- **Vector Database (FAISS / MongoDB / Chroma â€“ based on config)**
- **LLM (Groq / OpenAI / HuggingFace)**
- **PDF loaders**
- **Dotenv for secrets**

---

## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/product-catalog-rag.git
cd product-catalog-rag

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Environment Variables

Create a .env file in the root directory:

GROQ_API_KEY=your_api_key_here
OPENAI_API_KEY=your_api_key_here
MONGODB_URI=your_mongodb_uri_here


âš ï¸ Never commit .env to GitHub

ğŸ“¥ Data Ingestion

Place your product catalog PDFs in the configured data folder and run:

python ingestion.py


This will:

Load PDFs

Split text into chunks

Generate embeddings

Store them in the vector database

ğŸ’¬ Run the Application
python app.py


Ask questions like:

â€œList available LED downlightsâ€

â€œWhat is the wattage of model LD98?â€

â€œShow slim square downlighter detailsâ€

ğŸ§  How It Works (RAG Flow)

User asks a question

Question is converted to embeddings

Relevant chunks are retrieved from vector DB

LLM generates an answer only from retrieved context

Page numbers are included (if available)
